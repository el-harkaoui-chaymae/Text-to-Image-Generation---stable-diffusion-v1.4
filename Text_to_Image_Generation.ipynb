{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EL HARAKAOUI Chaymae --- 11-08-2024**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h1 style=color:red;>Text to Image Generation</h1></u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h2 style=color:#00AAFF>Hugging Face</h3></u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h3 style=color:#00B496>What is Hugging Face ?</h3></u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hugging Face** is a **company** - **platform** and an **open-source community** that specializes in **machine learning** - particularly in **natural language processing** - NLP.<br><br> It is widely known for its development of **Transformers** - an open-source library that provides a large collection of pre-trained models for various NLP tasks - including text classification - translation - summarization - question answering ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hugging_face_platform.png](hugging_face_platform.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h5 style=color:violet>Key Aspects of Hugging Face</h5>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![key_features_HF.png](key_features_HF.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h3 style=color:#00B496>Why are we using Hugging Face Here ?</h3></u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the pre-trained model **StableDiffusionPipeline** from Hugging Face's model hub. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h3 style=color:#00B496>What is authentication token ?</h3></u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " An **Authentication token** typically a **string** that serves as a **secure key** allowing you to **access** certain resources - services - data that require authentication. <br><br>Specifically - in this case - it is used to authenticate our access to models and datasets hosted on Hugging Face's model hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face hosts a variety of pre-trained models - some of which are **restricted** or **require** a Hugging Face account to access. These models are **not always publicly available** due to licensing - usage restrictions or because they are under active development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we **create** an **account** on Hugging Face - we can **generate** an **API key** also known as an **authentication token**. <br>This key is unique to our account and can be **used in scripts and applications** to **authenticate our access**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use the **authentication token** in our code - it tells the Hugging Face **API** that the **request** is coming from an **authenticated user**.<br >The **API** then **checks** if **the user associated with that token** has the **necessary permissions** to **access the requested resource**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h3 style=color:#00B496>How to get an authentication token ?</h3></u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1** - Create a Hugging Face Account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2** - Go to your account settings and look for the section labeled **Access Tokens**.<br><br>\n",
    "![access_tokens_section.png](access_tokens_section.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3** - Click on the **create new token** button to start the process of generating a new token.<br><br> \n",
    "![create_new_token.png](create_new_token.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4** - Once the token is generated -it will be displayed on the screen. Copy this token immediately - as it may not be displayed again for security reasons.<br><br>\n",
    "my generated token is : **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**<br><br>\n",
    "![generated_token.png](generated_token.png) <br><br>\n",
    "![my_GT.png](my_GT.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h3 style=color:#00B496>How to use the Generated authentication token in our code ?</h3></u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_auth_token = \"my_huggingface_api_token_here\"\n",
    "# pipe = StableDiffusionPipeline.from_pretrained( modelid , revision=\"fp16\", torch_dtype=torch.float16 , use_auth_token = my_auth_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h2 style=color:#00AAFF>Necessary Librairies</h2></u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "import customtkinter as ctk\n",
    "\n",
    "from PIL import ImageTk\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import autocast\n",
    "\n",
    "\n",
    "from diffusers import StableDiffusionPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h3 style=color:orange>PyTorch</h3></u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pytorch.png](pytorch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PyTorch** is an open-source **deep learning framework** primarily developed by **Facebook's AI Research lab**. It provides a flexible **platform** for building and training **neural networks**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h4 style=color:magenta>Key Features</h4></u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>Dynamic Computational Graph </font>** <br>\n",
    "\n",
    "PyTorch uses a **dynamic computation graph** also known as **define-by-run** - which means that the **graph** is **built** on-the-fly as **operations** are executed. <br>This makes it easier to modify and experiment with models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>Automatic Differentiation </font>** <br>\n",
    "\n",
    "PyTorch's **autograd** module automatically **computes gradients** - which are essential for training neural networks using **optimization algorithms** like **gradient descent**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>Tensor Computation</font>** <br>\n",
    "\n",
    " PyTorch supports powerful tensor computation - similar to NumPy - but with **GPU** acceleration. This makes it highly efficient for large-scale computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>Rich Ecosystem</font>** <br>\n",
    "\n",
    " PyTorch has a growing ecosystem - including libraries for vision - **TorchVision** - natural language processing - **TorchText** - reinforcement learning - **TorchRL** and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**<font color=blue>Interoperability</font>** <br> \n",
    "\n",
    "PyTorch **integrates** well with Python and **other** scientific computing **libraries **- making it easy to incorporate into existing projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>Community and Support</font>** <br> \n",
    "\n",
    "PyTorch has a large and active **community** - with extensive **documentation** - tutorials  - forums for support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h4 style=color:magenta>Encountered Error When Installing PyTorch</h4></u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=red> OSError: [WinError 126] The specified module could not be found. Error loading \"c:\\Users\\MTechno\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies.</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h4 style=color:magenta>Solution : Install Visual Studio Installer - Visual Studio C ++ Compiler</h4></u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=salmon><u>What is Visual Studio Installer ?</u></font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visual Studio Installer** is a **tool** provided by **Microsoft** that allows users to **install** - **update** and **manage** various **components** of **Visual Studio** - IDE. It provides a **streamlined way** to **select** the **necessary workloads** - **tools**  - **libraries** based on the type of development we plan to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Visual Studio Installer is a **background** utility tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=salmon><u>How Visual Studio Installer Manages Dependencies ?</u></font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>Selection of Workloads and Components</font>**<br>\n",
    "\n",
    "When we **run** the **Visual Studio Installer** - we are presented with various **workloads** - ex: Desktop Development with C++ - Web Development .... and **individual components**.<br>\n",
    "**Selecting** a **workload** or **component** that includes **tools** or **libraries** like **compilers** - **linkers** - **runtime libraries**  will **prompt** the **installer** to **download** and **install** those **necessary components** if **they are not already present** on our system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>Automatic Installation of Dependencies</font>**<br>\n",
    "\n",
    "The **installer** ensures that the **necessary dependencies** for the **selected** workloads and components are **installed**. This includes : <br>**Compilers**  - **Linkers** - **Build Tools** - **Libraries** - **SDKs**  ...... <br>\n",
    "For example - if we choose to install the **C++ development workload** - the **installer** will also **install** related **tools** and **libraries** needed to **build** and **run** C++ **applications**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>Updating and Patching</font>**<br>\n",
    "\n",
    "The Visual Studio **Installer** can also **update** existing **installations** of **Visual Studio** and its **components**. <br> It **checks** for **updates** and **patches** to ensure that your **development environment** is **up-to-date**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>Configuration</font>**<br>\n",
    "\n",
    "During the installation - the installer **configures** environment variables - paths - other settings **required** for the **tools** to **function correctly**. <br>This includes updating the system PATH to include directories where the necessary binaries and libraries are located."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=salmon><u>Examples of What the Installer Manages</u></font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=navy>Compilers and Linkers</font>**<br> \n",
    "\n",
    "Ensures that the required **compiler**  - ex : MSVC - and **linker** tools are installed and available.\n",
    "\n",
    "**<font color=navy>Runtime Libraries</font>**<br> \n",
    "\n",
    "Installs necessary runtime libraries like **the Visual C++ Redistributable packages**.\n",
    "\n",
    "**<font color=navy>SDKs and Libraries</font>**<br>\n",
    "\n",
    "Installs **Software Development Kits** - SDKs - and other **libraries** needed for specific development scenarios - ex: .NET SDKs for .NET development - DirectX SDKs for game development ...\n",
    "\n",
    "**<font color=navy>Build Tools</font>**<br>\n",
    "\n",
    "Includes **tools** required for building and debugging code such as **build systems** and **debugging tools**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=salmon><u>Why Did the Error Disappear After Installing the VS Code Installer - C++ Compiler ?</u></font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error we encountered - **OSError: [WinError 126] The specified module could not be found** - was due to **missing dependencies** that are **required** by the **fbgemm.dll** file in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VS Code** itself is an Integrated Development Environment - **IDE** - for **editing** and **running** code - but it **doesn't come** with the **underlying system libraries** and **runtime dependencies** required by certain **external packages** such as PyTorch - which **rely on native C++ libraries**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **fbgemm.dll** file in PyTorch has **dependencie**s on certain DLLs - **Dynamic Link Libraries** - provided by the **Visual C++ Redistributable packages**. Without these dependencies - Windows was **unable to load fbgemm.dll** resulting in the WinError 126 we encountered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of the error encountered with PyTorch related to **missing DLLs like fbgemm.dll** - the **Visual Studio Installer** likely **installed** the Microsoft Visual **C++ compiler** and associated components that PyTorch depends on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the **VS Code C++ compiler** is installed - it also installed the necessary **Visual C++ Redistributable packages** as part of the setup.<br>\n",
    "These **redistributables** provided the **missing DLLs** that **fbgemm.dll** and potentially other parts of PyTorch were **depending on**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the required **<font color=blue>DLLs</font>** now **present** on our system - Windows could successfully **load fbgemm.dll** when we imported PyTorch and the error was resolved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h4 style=color:magenta>What Are Runtime Librairies ?</h4></u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Runtime libraries** are **collections** of **pre-compiled routines** as functions - classes - data structures - that a **program** can call **during execution** - even though they are **not part of the program's original code**. These libraries provide **standard functionality** that is used by many programs and ensure that programs can be executed correctly on a system.<br><br>\n",
    "\n",
    "**<font color=olive>Example Scenario</font>**<br>\n",
    "\n",
    "Suppose we write a **C++ program** that uses the **printf() function** to print to the console. Instead of writing the printf() function ourself - we rely on the **implementation** of printf() **provided** in the **C runtime library** - CRT.\n",
    "When we compile the **program** - the **C runtime library** is **linked** with our **program**. If we dynamically link the CRT - our program will **call** printf() from the **MSVCP140.dll** library at **runtime**. If the system where our program is executed does not have this runtime library - our program might fail to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h4 style=color:magenta>What Are Visual C++ Redistributable packages ?</h4></u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visual C++ Redistributable Packages** are **runtime libraries** that are **required** to **run applications** built using **Microsoft Visual C++**. These packages contain the **necessary components** needed to execute programs that have been developed with **Microsoft’s Visual C++ tools**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h4 style=color:magenta>What Are Dependencies ?</h4></u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies refer to **external** **<font color=green>components</font>** - **<font color=green>libraries</font>** - **<font color=green>modules</font>** that a **software program** or **system** requires in order to **function correctly**. These dependencies provide necessary **functionalities** or **resources** that the main program itself does not include."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dependencies** are **external** in the sense that they are **not part** of our project's **core codebase** - we did not write them - but come from **third-party sources**. Where they are stored depends on how we manage them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=olive>Where the Dependencies could be located ?</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=turquoise>Stored in the Project Folder</font>**\n",
    "\n",
    "In some cases - these external **dependencies** are installed **locally in our project directory**. Although they are external to our code - meaning we didn't write them - they are **downloaded** and **stored** within our **project's folder structure** - such as in a **venv/ - Python - folder**. This makes them **external** but **local**.<br>\n",
    "\n",
    "When we use a **package manager** like npm - pip - yarn - these tools will **download** the required **external libraries** and **store** them **within the project folder** or a designated location like a virtual environment. This allows the project to be self-contained and others who work on the project can simply **install** these **dependencies** by running the appropriate commands - e.g. : npm install or pip install -r requirements.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=turquoise> Truly External - Not Stored in the Project Folder</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other cases - the external dependencies may not physically reside within your project folder. Instead they may be **installed globally on your system** or **exist in some centralized location** or **referenced from another location as if installed or hosted on the web**. In this scenario - the project folder contains no actual code for these external libraries - it only **references them** and the system **looks for them** in **predefined locations**  - ex: system paths or global package directories or URLs ....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=olive>Types of Dependencies</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=turquoise>+ Library Dependencies</font>**<br><br>\n",
    "\n",
    "**<font color=violet>Static Libraries</font>** <br><br>These are **compiled** directly into the application at **build time**. The **code** from the **library** becomes **part** of the **final executable**.<br>\n",
    "\n",
    "During the **compilation process** - the code from static libraries is **copied** directly into the **executable** - all the code needed from the static library is **included** in our **final executable file**.<br>\n",
    "\n",
    "SL are included in the executable at **compile time** - **no need** for the **library** file at **runtime**.<br><br>\n",
    "\n",
    "\n",
    "**<font color=violet>Dynamic Libraries</font>** <br><br>These are **separate files** - ex: **.dll in Windows** - **.so in Linux** - that are **loaded** into memory at **runtime**. The program will fail to run if the required dynamic library is missing or incompatible.<br>\n",
    "\n",
    "DL are **loaded** into **memory** when the application **runs**. The **executable** contains **references** to the dynamic libraries it needs - but the actual library **code** is **loaded** from the file at **runtime**.<br>\n",
    "\n",
    "At **runtime** - the **dynamic library** must be **available** in the system’s library path or in a **location** where the application **expects to find it**. If the library is not found - the application might fail to start or may not work correctly.<br>\n",
    "\n",
    "DL are loaded at **runtime** - the **library file** must be **accessible** when the application is **running**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=turquoise>+ Framework Dependencies</font>**<br>\n",
    "\n",
    "Some applications depend on larger frameworks like .NET - Java - Angular. These frameworks provide the foundation on which the application is built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=turquoise>+ System Dependencies</font>**<br>\n",
    "\n",
    "Programs may depend on specific system **components** like **drivers** or core OS **services**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=turquoise>+ Hardware Dependencies</font>**<br>\n",
    "\n",
    "Some software requires specific **hardware components** to function such as **GPUs** for AI/ML workloads or specialized **sensors** for IoT devices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h3 style=color:orange>autocast</h3></u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**autocast** is a **feature** from the **torch.cuda.amp** - **Automatic Mixed Precision module** in PyTorch - which is used to **automatically switch between different data types** - specifically between float32 and float16 - to optimize performance on modern **GPUs**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h4 style=color:magenta>Key Features</h4></u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=turquoise>Mixed Precision Training</font>**<br> \n",
    "\n",
    "It enables **mixed precision training** - where parts of the model are run in lower precision - float16 - while others remain in higher precision - float32. This can greatly improve performance without significantly affecting model accuracy.\n",
    "\n",
    "**<font color=turquoise>GPU Efficiency</font>**<br> \n",
    "\n",
    "Mixed precision helps leverage Tensor Cores available on NVIDIA GPUs - which are optimized for operations on half-precision floating-point numbers - making computations faster and more efficient.\n",
    "\n",
    "\n",
    "**<font color=turquoise>Automatic Casting</font>**<br>\n",
    "\n",
    " autocast **automatically** selects the **appropriate precision** for different **operations**. For example - matrix multiplications might run in float16 -while reductions - which could be more sensitive to precision - may run in float32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h3 style=color:orange>diffusers</h3></u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**diffusers** is an **open-source library** developed by **Hugging Face** that provides a unified interface for working with **diffusion models**. **Diffusion models** are a **class of generative models** that **create data** like images - audio - text --- by progressively **refining random noise into meaningful content through a diffusion process**. The diffusers library aims to make it easy to **implement** - **train** - **use** these models - particularly in **text-to-image generation** tasks like **Stable Diffusion**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h3 style=color:orange>Stable Diffusion</h3></u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stable Diffusion** is a type of **generative model** primarily used for creating **images** based on **text prompts**. It’s part of a broader category of models known as **diffusion models**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diffusion models** are inspired by the process of **diffusion** in **physics** - where **particles spread out over time**. In the context of **generative modeling** - they use a similar concept but in **reverse**. The idea is to gradually **convert noise** into a **structured image** through a series of steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **<font color=blue>Training Phase</font>**<br><br>\n",
    " \n",
    "During **training** - the model learns to **reverse** a **diffusion process**. This involves two main stages : <br><br>\n",
    "\n",
    "**<font color=navy>+ Forward Diffusion Process</font>**<br> \n",
    "\n",
    "The model gradually adds **noise to images in the dataset** over many steps until the images are completely noisy and indistinguishable from **random noise**. This process is designed to teach the model how noise corrupts images.\n",
    "\n",
    "**<font color=navy>+ Reverse Diffusion Process</font>**<br>\n",
    "\n",
    "The model then learns to reverse this process. It is trained to **denoise the noisy images** step-by-step - eventually **reconstructing the original images from noisy versions**. The model learns this by comparing its denoised outputs with the original images and adjusting its parameters to minimize the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  **<font color=blue>Generative Phase</font>**<br><br>\n",
    "\n",
    " \n",
    "Once trained - the Stable Diffusion model can generate new images from scratch based on text prompts :\n",
    "\n",
    "**<font color=navy>+ Starting with Noise</font>**<br> \n",
    "\n",
    "To create a new image - the model **begins** with a **random noise pattern**.\n",
    "\n",
    "**<font color=navy>+ Guided Denoising</font>**<br>\n",
    "\n",
    "It then iteratively applies a **series of transformations** to this **noise** - **guided by the text prompt**. These transformations involve denoising the image progressively while incorporating the details from the text description.\n",
    "\n",
    "**<font color=navy>+ Final Image</font>**<br>\n",
    "\n",
    "After several iterations - the **noise** is **transformed** into a **coherent image** that **aligns** with the **text prompt**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  **<font color=blue>Text-to-Image Synthesis</font>**<br><br>\n",
    "\n",
    "\n",
    "The Stable Diffusion model is particularly notable for its ability to generate images based on text prompts. This is achieved through a **combination** of :\n",
    "\n",
    "**<font color=navy>+ Text Encoder</font>**<br>  \n",
    "\n",
    "A **neural networ**k that processes and **converts** the **text prompt** into a **feature vector**.\n",
    "\n",
    "**<font color=navy>+ Conditioning Mechanism</font>**<br> \n",
    "\n",
    "This **feature vector** is used to **condition** the **denoising process** - guiding the model to generate an image that **matches** the description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h3 style=color:orange>StableDiffusionPipeline</h3></u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **StableDiffusionPipeline** is a **pipeline class** provided by the **diffusers library** that organizes the **components** of the **Stable Diffusion model**  for easier use. <br>\n",
    "\n",
    "This class wraps the entire workflow for **image generation** from **text prompts.** It abstracts away the complexity and provides an easy-to-use **interface** for interacting with the model.<br>\n",
    "\n",
    "It contains various components such as the pre-trained text **encoder** - CLIP - UNet used for **denoising** - **scheduler** for controlling the **diffusion process** - VAE **Variational Autoencoder** used for encoding and decoding **latent images**.<br>\n",
    "\n",
    "Inside the **pipeline** - the actual **Stable Diffusion model** is the key **generative model** responsible for creating the images. The pipeline integrates this model with other **supporting pieces**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we say that **StableDiffusionPipeline** is a **pipeline** - we mean that it is a **structured workflow** that **integrates several components** to achieve a **specific task** - in this case - **generating images from text prompts**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h4 style=color:magenta>what is a PipeLine ?</h4></u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning and deep learning - a **pipeline** refers to a **sequence** of data processing **steps** or **stages** that are **applied** in a **specific order** to produce a desired **outcome**. Each stage in the pipeline typically performs a **specific task** - and the **output** of one stage becomes the **input** for the next stage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a machine learning pipeline - the typical steps might include : <br>\n",
    "\n",
    "**<font color=navy>1 - Data Preprocessing</font>** : Cleaning and transforming raw data - e.g. handling missing values - scaling ...<br>\n",
    "**<font color=navy>2 - Feature Engineering</font>** : Creating new features or selecting important features.<br>\n",
    "**<font color=navy>3 - Model Training</font>** : Fitting a machine learning model to the data.<br>\n",
    "**<font color=navy>4 - Evaluation</font>** : Assessing the performance of the model.<br>\n",
    "**<font color=navy>5 - Prediction</font>** : Making predictions based on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Context of StableDiffusionPipeline : <br>\n",
    "\n",
    "The **StableDiffusionPipeline** is a **specific pipeline** designed for **text-to-image generation**. It orchestrates the following components in sequence :\n",
    "\n",
    "**<font color=blue>Text Encoder</font>** : Converts the **input text** - prompt - into a **latent representation**.<br>\n",
    "**<font color=blue>UNet Model</font>** : Uses this **latent representation** to progressively refine an image by **reducing noise** in a **diffusion process**.<br>\n",
    "**<font color=blue>Scheduler</font>** : Controls how the **denoising process** happens over time.<br>\n",
    "**<font color=blue>VAE Decoder</font>** : Converts the final **latent image** back into **pixel space** to produce a visible image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h3 style=color:orange> transformers</h3></u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Hugging Face transformers library** is an open-source Python library that provides **access** to a wide range of pretrained **Transformer models** for various **NLP** tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h4 style=color:magenta>Key Features</h4></u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=olive>Pretrained Models</font>**<br> \n",
    "\n",
    "\n",
    "Hugging Face offers thousands of **pretrained models** for various tasks such as **text classification** - **question answering** - **summarization** - **translation** - **text generation**  and more. These models can be used **directly** or **fine-tuned** on custom datasets.\n",
    "\n",
    "**<font color=olive>Easy-to-Use API</font>**<br> \n",
    "\n",
    "\n",
    "The library provides a simple **API** to **load** - **use** - **fine-tune** models with just a few lines of code. It abstracts many of the complexities of dealing with machine learning models.\n",
    "\n",
    "\n",
    "**<font color=olive>Task-Specific Pipelines</font>**<br>\n",
    "\n",
    "\n",
    "The library includes high-level **pipelines** that allow users to perform common **NLP tasks** out of the box without needing to understand the inner workings of the models.\n",
    "\n",
    "\n",
    "**<font color=olive>Fine-Tuning</font>**<br>\n",
    "\n",
    "\n",
    "Hugging Face makes it straightforward to **fine-tune pretrained models** on **custom datasets** - enabling users to tailor models to their **specific** needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h3 style=color:orange>tokenizer</h3></u>** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **tokenizer** is a crucial component in NLP models - particularly in models based on the **Transformer architecture**. The tokenizer **converts** raw text like sentences or paragraphs into a **structured format** that a model can understand and process.<br>\n",
    "\n",
    "**Transformers** and most **NLP models** work with **numerical representations** of **text**. Since models **cannot** directly **interpret** plain **text** - a **tokenizer** transforms words or characters into **tokens** - which are **numerical indices or vectors**. These tokens are then fed into the model.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h2 style=color:#00AAFF>Create the graphical interface</h2></u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the root\n",
    "\n",
    "root = tk.Tk()\n",
    "root.geometry(\"600x700\")\n",
    "root.title(\"Text To Image Generation\")\n",
    "\n",
    "# text prompt\n",
    "\n",
    "prompt_text = ctk.CTkEntry(master=root, height=40, width=580, font=(\"Arial\", 20), text_color=\"black\", fg_color=\"white\", border_width=2,border_color=\"navy\")\n",
    "prompt_text.place(x=10, y=10)\n",
    "\n",
    "\n",
    "# a Label that contains the image\n",
    "\n",
    "label_image = ctk.CTkLabel(master=root, height=570, width=580,fg_color=\"navy\",text='')\n",
    "label_image.place(x=10, y=118)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h2 style=color:#00AAFF>Load the Stable Diffusion pipeline locally</h2></u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MTechno\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\diffusers\\pipelines\\pipeline_loading_utils.py:219: FutureWarning: You are loading the variant fp16 from CompVis/stable-diffusion-v1-4 via `revision='fp16'` even though you can load it via `variant=`fp16`. Loading model variants via `revision='fp16'` is deprecated and will be removed in diffusers v1. Please use `variant='fp16'` instead.\n",
      "  warnings.warn(\n",
      "safety_checker\\model.safetensors not found\n",
      "Keyword arguments {'use_auth_token': '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'} are not expected by StableDiffusionPipeline and will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2798fe5a42e046469646c2452e91b59f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch C:\\Users\\MTechno\\.cache\\huggingface\\hub\\models--CompVis--stable-diffusion-v1-4\\snapshots\\2880f2ca379f41b0226444936bb7a6766a227587\\unet: Error no file named diffusion_pytorch_model.safetensors found in directory C:\\Users\\MTechno\\.cache\\huggingface\\hub\\models--CompVis--stable-diffusion-v1-4\\snapshots\\2880f2ca379f41b0226444936bb7a6766a227587\\unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch C:\\Users\\MTechno\\.cache\\huggingface\\hub\\models--CompVis--stable-diffusion-v1-4\\snapshots\\2880f2ca379f41b0226444936bb7a6766a227587\\vae: Error no file named diffusion_pytorch_model.safetensors found in directory C:\\Users\\MTechno\\.cache\\huggingface\\hub\\models--CompVis--stable-diffusion-v1-4\\snapshots\\2880f2ca379f41b0226444936bb7a6766a227587\\vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "c:\\Users\\MTechno\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the Stable Diffusion pipeline locally\n",
    "\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=\"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\")\n",
    "\n",
    "# Save the model to the project directory\n",
    "pipe.save_pretrained(\"./stable_diffusion_v1_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h4 style=color:magenta>from pretrained method</h4></u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the **from_pretrained** is a **method** of the class **StableDiffusionPipeline** from the librairy **diffusers** - it is used to **load** a **pre-trained diffusion model** like **Stable Diffusion** and its associated **components** from the **Hugging Face Model Hub** or a **local directory**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=olive>What from_pretrained Does ?</font>**<br><br>\n",
    "\n",
    "**<font color=brown>Download Model Weights and Components</font>**<br>\n",
    "\n",
    "If the model is not already **stored locally** - from_pretrained **downloads** the pre-trained model **weights** - **configuration files** and any other necessary components such as **tokenizers** - **schedulers** or **safety filters** from the **Hugging Face Model Hub**.\n",
    "\n",
    "**<font color=brown>Initialize the Pipeline</font>**<br>\n",
    "\n",
    "It **initializes** the **pipeline** with all the components needed to **run** the model. For StableDiffusionPipeline - this typically includes the **U-Net model** - the **variational autoencoder** - VAE - the **text encoder** and the **scheduler**. These components are essential for generating images from text prompts using the Stable Diffusion model.\n",
    "\n",
    "**<font color=brown>Configure the Model</font>**<br>\n",
    "\n",
    "The method allows us to pass various options - such as :<br>\n",
    "- **revision** : To specify a particular version of the model.\n",
    "- **torch_dtype** : To set the data type for tensors - ex: torch.float16 for half-precision - which can save memory and speed up computations.\n",
    "- **use_auth_token** : To authenticate and download private models that require access permissions.\n",
    "- **device** : To specify where the model should run - ex: CPU or GPU.\n",
    "\n",
    "**<font color=brown>Cache the Model Locally</font>**<br>\n",
    "\n",
    "Once the model is downloaded - it is **cached** locally on our machine. This means that **subsequent call**s to from_pretrained with the same model identifier will load the model from the **local cache** avoiding redundant downloads.\n",
    "\n",
    "**<font color=brown>Return a Ready-to-Use Pipeline</font>**<br>\n",
    "\n",
    "The method returns an **instance** of the **StableDiffusionPipeline** that is fully set up and ready to generate images from text prompts.<br>\n",
    "we can then use this pipeline to generate images - fine-tune the model or perform other tasks that the model is capable of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h4 style=color:magenta>The Downloaded model folder's structure</h4></u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model directory doesn’t contain **Python files** because it’s meant to store the **model's data** - not the code. The actual Python code is found in **external libraries** like **diffusers** or in **custom scripts** we write. This approach keeps the model files and code separate - allowing for flexibility - reusability - ease of distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SD_downloaded_pipeline.png](SD_downloaded_pipeline.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h2 style=color:#00AAFF>Load the Stable Diffusion pipeline from the local directory</h2></u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c8468ba323541baab538050e9e6ea6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StableDiffusionPipeline {\n",
       "  \"_class_name\": \"StableDiffusionPipeline\",\n",
       "  \"_diffusers_version\": \"0.30.0\",\n",
       "  \"_name_or_path\": \"./stable_diffusion_v1_4\",\n",
       "  \"feature_extractor\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPFeatureExtractor\"\n",
       "  ],\n",
       "  \"image_encoder\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"requires_safety_checker\": true,\n",
       "  \"safety_checker\": [\n",
       "    \"stable_diffusion\",\n",
       "    \"StableDiffusionSafetyChecker\"\n",
       "  ],\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"PNDMScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModel\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"unet\": [\n",
       "    \"diffusers\",\n",
       "    \"UNet2DConditionModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Stable Diffusion pipeline from the local directory\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"./stable_diffusion_v1_4\", torch_dtype=torch.float16)\n",
    "pipe.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h4 style=color:magenta>pipe.to( device )</h4></u>**<br>\n",
    "\n",
    "This line **transfers** the **entire model** including its weights and components to the specified device either **GPU** or **CPU**.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h2 style=color:#00AAFF>Define the Generate Image function</h2></u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the Generate_Image function\n",
    "\n",
    "def Generate_Image():\n",
    "    try:\n",
    "        with autocast(device):\n",
    "            result = pipe(prompt_text.get(), guidance_scale=8.5)\n",
    "            print(result)\n",
    "            image = result[\"images\"][0] \n",
    "            \n",
    "            for i in len(result) :\n",
    "                print(result[\"images\"][i])\n",
    "        \n",
    "        # Resize the image to fit the label dimensions\n",
    "        image = image.resize((label_image.winfo_width(), label_image.winfo_height()), Image.ANTIALIAS)\n",
    "        \n",
    "        # Save and display the image\n",
    "        image.save('Generated_Image.png')\n",
    "        img = ImageTk.PhotoImage(image)\n",
    "        label_image.configure(image=img)\n",
    "        label_image.image = img \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h4 style=color:magenta>result = pipe( prompt_text.get() , guidance_scale = 8.5 )</h4></u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**guidance_scale** is a **parameter** from **1** to **10** that controls how **strongly** the model should **follow** the given **text prompt** during image generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models** that **generate images** - especially in the context of tasks like text-to-image synthesis - often **return multiple images**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**result** is typically a **dictionary**. This dictionary contains **various** pieces of information - one of which is a **list of images**. The exact structure of result would depend on the implementation of pipe - but in this context  :<br>\n",
    "\n",
    "- result is a **dictionary**.<br>\n",
    "- One of the keys in the dictionary is - **images**.<br>\n",
    "- The value associated with the - **images** - key is a **list** - and the **items** in this list are usually **image objects**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h2 style=color:#00AAFF>Define the generate_image button<h2></u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Image button\n",
    "\n",
    "generation_btn = ctk.CTkButton(master=root, height=40, width=120, font=(\"Arial\", 20), text_color=\"white\", fg_color=\"navy\", command=Generate_Image)\n",
    "generation_btn.configure(text=\"Generate Image\")\n",
    "generation_btn.place(x=210, y=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the GUI loop\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h2 style=color:#00AAFF>The Whole Code</h2></u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c2d3e28a0040baa18be6fbfbb5dc07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec6afa334e2a43ba8c2186b41091ff04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MTechno\\AppData\\Local\\Temp\\ipykernel_19432\\405013505.py:46: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  image = image.resize((label_image.winfo_width(), label_image.winfo_height()), Image.ANTIALIAS)\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "import customtkinter as ctk\n",
    "from PIL import Image, ImageTk\n",
    "import torch\n",
    "from torch import autocast\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# the root\n",
    "\n",
    "root = tk.Tk()\n",
    "root.geometry(\"600x700\")\n",
    "root.title(\"Text To Image Generation\")\n",
    "\n",
    "# text prompt\n",
    "\n",
    "prompt_text = ctk.CTkEntry(master=root, height=40, width=580, font=(\"Arial\", 20), text_color=\"black\", fg_color=\"white\", border_width=2,border_color=\"navy\")\n",
    "prompt_text.place(x=10, y=10)\n",
    "\n",
    "\n",
    "# a Label that contains the image\n",
    "\n",
    "label_image = ctk.CTkLabel(master=root, height=570, width=580,fg_color=\"navy\",text='')\n",
    "label_image.place(x=10, y=118)\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    " \n",
    "# Load the Stable Diffusion pipeline from the local directory\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"./stable_diffusion_v1_4\", torch_dtype=torch.float16)\n",
    "pipe.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Define the Generate_Image function\n",
    "\n",
    "def Generate_Image():\n",
    "    try:\n",
    "        with autocast(device):\n",
    "            result = pipe(prompt_text.get(), guidance_scale=8.5)\n",
    "            image = result[\"images\"][0]  \n",
    "        \n",
    "        # Resize the image to fit the label dimensions\n",
    "        image = image.resize((label_image.winfo_width(), label_image.winfo_height()), Image.ANTIALIAS)\n",
    "        \n",
    "        # Save and display the image\n",
    "        image.save('Generated_Image.png')\n",
    "        img = ImageTk.PhotoImage(image)\n",
    "        label_image.configure(image=img)\n",
    "        label_image.image = img \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Generate Image button\n",
    "\n",
    "generation_btn = ctk.CTkButton(master=root, height=40, width=120, font=(\"Arial\", 20), text_color=\"white\", fg_color=\"navy\", command=Generate_Image)\n",
    "generation_btn.configure(text=\"Generate Image\")\n",
    "generation_btn.place(x=210, y=60)\n",
    "\n",
    "# Start the GUI loop\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><h2 style=color:#00AAFF>Encountered Errors</h2></u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3 style=color:red>Cannot initialize model with low cpu memory usage because accelerate was not found in the environment. Defaulting to low_cpu_mem_usage=False. It is strongly recommended to install accelerate for faster and less memory-intense model loading.</h3>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3 style=color:red>safety_checker\\model.safetensors not found.</h3>**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
